- 9/14 공부 내용
  - 코드 refactoring 하는 중
    - 소스 : https://www.kaggle.com/tmheo74/3rd-ml-month-12th-solution
  - 질문 사항
    - AdamW
      - closure 역할이 뭔지 모르겠다
      - 왜 adam 은 sparse gradient를 처리할 수 없는가
      - exponential moving average 처리는 왜 하는거지
    - CosineAnnealingWithRestartsLR
  - 알게된 내용
    - AdamW
      - weight decay regularization 이 뭔가? 이걸 왜 adam 방식에 적용하려 하는건가?
        - 참조 : https://hiddenbeginner.github.io/deeplearning/paperreview/2019/12/29/paper_review_AdamW.html
        - weight 값을 조절해서, 오버피팅을 막기 위함 
          - regularization과 유사함
          - 단, Adam의 경우엔 regularization과 동일하다고 할 수 없다
    - CosineAnnealingWithRestartsLR
      - generalization : train/test 에서의 성능 차이가 적을수록, generalization이 잘 이뤄졌다고 한다
      - 이 generalization을 촉진하기 위해, learning rate를 주기적으로 증폭시켜준다
        - 이렇게 함으로서, local maxima에 빠지는 것을 방지한다